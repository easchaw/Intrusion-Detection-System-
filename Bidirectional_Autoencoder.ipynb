{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional Autoencoder Anomaly Detection- Sequential Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dependencies\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import random\n",
    "from random import shuffle\n",
    "import os, time, glob\n",
    "import itertools\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from numpy import mean,std\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Input,regularizers\n",
    "from keras import optimizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import LSTM,CuDNNLSTM,Embedding,Bidirectional, Concatenate\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import mean,std\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from math import sqrt\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.convolutional import Conv1D,MaxPooling1D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure GPU settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "config = tf.ConfigProto()\n",
    "\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract the normal training data\n",
    "start_time=time.time()\n",
    "Training_input=[]\n",
    "Test_input=[]\n",
    "\n",
    "\n",
    "root=('C://Users//a.chawla//Desktop//IDS//ADFA-LD//ADFA-LD//Validation_Data_Master')\n",
    "os.chdir(root)\n",
    "\n",
    "data=[]\n",
    "\n",
    "#print(os.listdir(root))\n",
    "for fname in glob.glob(\"*\"):\n",
    "    file = open(fname,\"r\",encoding=\"utf8\")                     ## open and read the file \n",
    "    callSeqString = (file.read())\n",
    "    callSeq = callSeqString.split()            ## Split the characters \n",
    "    callSeqInt = list(map(int, callSeq))       ## Convert Strings to Integers\n",
    "    data.append([callSeqInt])\n",
    "    \n",
    "\n",
    "\n",
    "root=('C://Users//a.chawla//Desktop//IDS//ADFA-LD//ADFA-LD//Training_Data_Master')\n",
    "os.chdir(root)\n",
    "\n",
    "\n",
    "#print(os.listdir(root))\n",
    "for fname in glob.glob(\"*\"):\n",
    "    file = open(fname,\"r\", encoding=\"utf8\")                     ## open and read the file\n",
    "    #print(file)\n",
    "    callSeqString = file.read()\n",
    "    callSeq = callSeqString.split()            ## Split the characters \n",
    "    #print(callSeq)\n",
    "    callSeqInt = list(map(int, callSeq))       ## Convert Strings to Integers\n",
    "    #print (callSeqInt)\n",
    "    data.append([callSeqInt])\n",
    "\n",
    "shuffle(data)\n",
    "Training=[]\n",
    "for i in range(0,len(data)) :\n",
    "        Training.append(data[i][0])\n",
    "\n",
    "\n",
    "Training_input=Training[0:3469]\n",
    "Test_input=Training[3469:5205]\n",
    "print('Length of Training Input:',len(Training_input))\n",
    "print('Length of Test Input:',len(Test_input))\n",
    "\n",
    "\n",
    "print(time.time()-start_time)\n",
    "Training_input.sort(key=len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence( input, seqLength):\n",
    "    print('original length of input:',len(input))\n",
    "    print('Start Splitting the Sequence')\n",
    "\n",
    "    length=seqLength\n",
    "    OutSeq=list()\n",
    "\n",
    "    for i in range(0,len(input)):\n",
    "        \n",
    "        startIndex=0\n",
    "        endIndex=length\n",
    "        currentseq=input[i]\n",
    "       \n",
    "        print('Actual length of the input is:',len(currentseq))\n",
    "        TotalBatches= int(len(currentseq)/length)\n",
    "        remainder= int(len(currentseq)% length)\n",
    "        print('Number of splits:',TotalBatches)\n",
    "        print('********************')\n",
    "\n",
    "    \n",
    "        for batches in range(0,TotalBatches):\n",
    "            #print(startIndex,endIndex)\n",
    "            batchseq=currentseq[startIndex:endIndex]\n",
    "            OutSeq.append(batchseq)\n",
    "    \n",
    "            startIndex=startIndex+ length\n",
    "            endIndex=endIndex+length\n",
    "\n",
    "        if(remainder>0):\n",
    "            laststartIndex= TotalBatches * length\n",
    "            lastendIndex= laststartIndex + remainder\n",
    "            print('Remainder series:')\n",
    "            print(laststartIndex,':', lastendIndex)\n",
    "            print('-----------------------------------------------')\n",
    "            OutSeq.append(currentseq[laststartIndex:lastendIndex])\n",
    "    return (OutSeq)\n",
    "\n",
    "Training_seq=split_sequence(Training_input, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create the Multilayer Perceptron model'''\n",
    "\n",
    "##Define the Bidirectional Autoencoder Model\n",
    "\n",
    "embed_size=64\n",
    "bn1 = BatchNormalization()    \n",
    "def define_models( vocab_size, n_units):\n",
    "\n",
    "# define training encoder\n",
    "    encoder_inputs = Input(shape=(None,),name=\"Encoder_input\")\n",
    "    Shared_Embedding = Embedding(input_dim=vocab_size,output_dim=embed_size,name=\"Embedding\")\n",
    "    word_embedding_context = Shared_Embedding((encoder_inputs))\n",
    "    \n",
    " \n",
    "    encoder = Bidirectional(CuDNNLSTM(n_units, return_state=True,\n",
    "                                  name='Bidirectional_lstm')) \n",
    "    \n",
    "    \n",
    "    ##Initialize RNN \n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = (encoder(bn1(word_embedding_context)))\n",
    "    encoder_outputs = Dropout(0.5)(encoder_outputs)\n",
    "    state_h = Concatenate(name=\"Merge_h_states\")([forward_h, backward_h])\n",
    "    state_c = Concatenate(name=\"Merge_c_states\")([forward_c, backward_c])\n",
    "    encoder_states = [state_h, state_c]\n",
    "  \n",
    "    \n",
    "# define training decoder\n",
    "    decoder_inputs = Input(shape=(None,),name=\"Decoder_input\")\n",
    "    decoder_lstm = CuDNNLSTM(n_units*2, return_sequences=True, return_state=True,\n",
    "                             kernel_initializer='glorot_uniform',\n",
    "                             recurrent_initializer='orthogonal',\n",
    "                             bias_initializer=\"zeros\",\n",
    "                             \n",
    "                             name=\"Decoder_lstm\")\n",
    "    \n",
    "    ##Initialize Decoder RNN with Embeddings\n",
    "    word_embedding_answer = Shared_Embedding(decoder_inputs)\n",
    "    decoder_outputs, _, _ = decoder_lstm(word_embedding_answer, initial_state=encoder_states)\n",
    "    \n",
    "    decoder_dense = Dense(vocab_size, activation='softmax' , name=\"Dense_layer\")\n",
    "    \n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = Dropout(0.2)(decoder_outputs)\n",
    "    \n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "     \n",
    "    # define inference encoder\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "     \n",
    "    # define inference decoder\n",
    "    decoder_state_input_h = Input(shape=(n_units*2,), name=\"H_state_input\")\n",
    "    decoder_state_input_c = Input(shape=(n_units*2,), name=\"C_state_input\")\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(word_embedding_answer, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "     \n",
    "# return all models\n",
    "    return model, encoder_model, decoder_model\n",
    "\n",
    "\n",
    "cardinality=341\n",
    "train, infenc, infdec = define_models(cardinality,200)\n",
    "train.compile(loss=\"categorical_crossentropy\",optimizer='adam',  metrics=['acc'])\n",
    "print(train.summary())\n",
    "\n",
    "plot_model(train, to_file='LSTM Based Autoencoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global variable declaration\n",
    "source=[]\n",
    "target=[]\n",
    "epochs=[]\n",
    "loss_plot=[]\n",
    "loss_val=[]\n",
    "\n",
    "\n",
    "## Batch Index Intialization Information\n",
    "startIndex=0\n",
    "batchsize=64\n",
    "endIndex=batchsize\n",
    "totalBatches= (int(len(Training_input)/batchsize))\n",
    "\n",
    "print(\"Total Number of Training input sequence(s):\",len(Training_input))\n",
    "print(\"Total number of Batches:\",totalBatches)\n",
    "print(\"===========================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global Source_train\n",
    "global target_train\n",
    "global shifted_target_train\n",
    "\n",
    "\n",
    "def train_epoch(x1,startIndex,endIndex):\n",
    "    losses = []\n",
    "    losses_val=[]\n",
    "    for batch_seq in range(totalBatches):\n",
    "        print(\"Start Index is:\",startIndex )\n",
    "        print(\"End Index is:\",endIndex )\n",
    "        print(\"===================================\")\n",
    "        print(\"Total Number of sequences processed in a batch:\",batchsize)\n",
    "    \n",
    "        batch_seq=Training_input[startIndex:endIndex]\n",
    "        max_length=max(len(seq) for seq in batch_seq)\n",
    "        train_seq=pad_sequences(batch_seq,max_length,padding='pre', dtype=int)\n",
    "        #print (train_seq)\n",
    "        temp_x=[]\n",
    "        temp_y=[]\n",
    "        temp_z=[]\n",
    "        accuracy = []\n",
    "        #result_array = np.array([])\n",
    "        for seq in train_seq:\n",
    "            \n",
    "            ##Encoder Input data-2D\n",
    "            source=seq[None,:]\n",
    "            #src_encoded = to_categorical(source,num_classes=cardinality)\n",
    "            #print('Source is:',src_encoded)\n",
    "            #print('Encoder input is:',source)\n",
    "            temp_x.append(source)\n",
    "            \n",
    "            \n",
    "            ##Decoder Input data- 2D\n",
    "            Decoder_input=seq[:-1]\n",
    "            shifted_target=np.concatenate(([0], Decoder_input))\n",
    "            shifted_target_input=shifted_target[None,:]\n",
    "            #target_seq = to_categorical(shifted_target_input,num_classes=cardinality)\n",
    "            #print('Shifted Target is:',target_seq)\n",
    "            temp_y.append(shifted_target)\n",
    "\n",
    "            ##decoder_target_data- 3D (one-hot encoded)\n",
    "            target=source\n",
    "            target_seq = to_categorical(target,num_classes=cardinality)\n",
    "            #print('Target is',target_seq)\n",
    "            temp_z.append(target_seq)\n",
    "            #print(temp_y)\n",
    "            #print(\"-----------------------\")\n",
    "   \n",
    "        \n",
    "        ##Reshape the arrays\n",
    "        Source_seq=np.array(temp_x).reshape(batchsize, max_length)\n",
    "        Source_train=Source_seq\n",
    "        #Source_seq_val=Source_seq[26:31]\n",
    "        #print('Source is:',Source_train)\n",
    "        \n",
    "        ##Decoder Input\n",
    "        shifted_target_seq=np.array(temp_y).reshape(batchsize, max_length)\n",
    "        shifted_target_train=shifted_target_seq\n",
    "        #shifted_target_val=shifted_target_seq[1:2]\n",
    "        #print('Decoder Input is:',shifted_target_train)\n",
    "        \n",
    "    \n",
    "        ##Decoder output\n",
    "        target_seq=np.array(temp_z).reshape(batchsize, max_length,cardinality)\n",
    "        target_train=target_seq\n",
    "        #print('Target is',target_train)\n",
    "        #print('Decoder output shape is:',target_train.shape)\n",
    "        #target_seq_val=target_seq[1:2]\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=0)\n",
    "    \n",
    "     ## Train in Minibatches\n",
    "        history=train.fit(x=[Source_train,shifted_target_train], y=target_train,shuffle= True, \n",
    "                          validation_split=0.02,\n",
    "                          callbacks=[es])\n",
    "        loss_train = history.history['loss']\n",
    "        val_loss_train = history.history['val_loss']\n",
    "        #loss = train.train_on_batch(x=[Source_train,shifted_target_train], y=target_train)\n",
    "        losses.append(loss_train)\n",
    "        losses_val.append(val_loss_train)\n",
    "\n",
    "        \n",
    "        \n",
    "    # Incrementing the batch Index number\n",
    "        startIndex=startIndex+batchsize\n",
    "        endIndex=endIndex+batchsize\n",
    "        \n",
    "        loss = np.mean(losses)\n",
    "        val_loss=np.mean(losses_val)\n",
    "        \n",
    "        train.reset_states()\n",
    "    return loss, val_loss\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs =40\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss,val_loss = train_epoch(Training_input,startIndex,endIndex)\n",
    "    print(\"Epoch\",epoch+1,\":\"  \" \"  \"loss:%.3f\" %(loss), \" - \"  \"Val_loss: %.3f\" %(val_loss))\n",
    "    print(\"----------------------------------------------------------------------------------\")    \n",
    "    epochs.append(epoch)\n",
    "    loss_plot.append(loss)\n",
    "    loss_val.append(val_loss)\n",
    "   \n",
    "    \n",
    "# Plot loss vs epochs\n",
    "plt.plot(epochs, loss_plot,'b',label='loss')    \n",
    "plt.plot(epochs, loss_val,'bo', label='Validation loss')\n",
    "plt.title('Loss and epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"----------------------------------\")    \n",
    "print(\"Total Execution Time to train: %.3f\" 'sec' % (time.time()-start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store Embeddings\n",
    "embeddings=train.layers[2].get_weights()[0]\n",
    "print('Shape of embedding vector',embeddings.shape) # shape: (vocab_size, embedding_dim)\n",
    "\n",
    "##We need to normalize the embeddings so that the dot product between two embeddings becomes the cosine similarity.\n",
    "\n",
    "embeddings = embeddings / np.linalg.norm(embeddings, axis = 1).reshape((-1, 1))\n",
    "embeddings[0][:10]\n",
    "np.sum(np.square(embeddings[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fetch Attack Data\n",
    "\n",
    "root=('C://Users//a.chawla//Desktop//IDS//ADFA-LD//Attack_Data_Master')\n",
    "os.chdir(root)\n",
    "\n",
    "evalprobSeq =[]\n",
    "label=[]\n",
    "\n",
    "for dirname in glob.glob(\"*\"):\n",
    "    os.chdir(os.path.join(root,dirname))           ## Dynamically change the directory name \n",
    "    for fname in glob.glob(\"*\"): \n",
    "        file = open(fname,\"r\")                     ## open and read the file\n",
    "        callSeqString = file.read()\n",
    "        callSeq = callSeqString.split()            ## Split the characters \n",
    "        callSeqInt = list(map(int, callSeq))      ## Convert Strings to Integers\n",
    "        evalprobSeq.append([callSeqInt])\n",
    "        label.append(1)\n",
    "        \n",
    "\n",
    "for j in Test_input:\n",
    "    label.append(0)\n",
    "    evalprobSeq.append([j])\n",
    "    \n",
    "    \n",
    "Testing_Input=[]\n",
    "\n",
    "for i in range(0,len(evalprobSeq)) :\n",
    "        Testing_Input.append(evalprobSeq[i][0])      \n",
    "        \n",
    "Attack_label = np.array(label)\n",
    "\n",
    "\n",
    "print('Length of Testing Input is:',len(Testing_Input))\n",
    "test= Testing_Input[0:1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to calculate the cosine similarity between call sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a,b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim\n",
    "def cosine_similarity(a,b):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(a)):\n",
    "        x = a[i]; y = b[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function call to calculate the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate target given source sequence\n",
    "\n",
    "def predict_TF(input):\n",
    "    # encode the sequence\n",
    "    pSum=0\n",
    "    encoded=np.array(input)[None, :]\n",
    "    state = infenc.predict(encoded)\n",
    "   \n",
    "    yhat, h, c = infdec.predict([encoded] + state)   \n",
    "    temp = yhat[0]\n",
    "  \n",
    "    for i in range(0, len(input)-1): \n",
    "        \n",
    "        # predict next word\n",
    "        next = input[i+1]\n",
    "        \n",
    "        # store prediction\n",
    "        p = -math.log10(temp[i][next])\n",
    "        pSum = pSum + p\n",
    "    return pSum/len(input)\n",
    "\n",
    "prob=[]\n",
    "for x in Testing_Input:\n",
    "    p = predict_TF(x)\n",
    "    prob.append(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
